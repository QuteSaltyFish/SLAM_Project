@article{Maturana2015,
abstract = {Robust object recognition is a crucial skill for robots operating autonomously in real world environments. Range sensors such as LiDAR and RGBD cameras are increasingly found in modern robotic systems, providing a rich source of 3D information that can aid in this task. However, many current systems do not fully utilize this information and have trouble efficiently dealing with large amounts of point cloud data. In this paper, we propose VoxNet, an architecture to tackle this problem by integrating a volumetric Occupancy Grid representation with a supervised 3D Convolutional Neural Network (3D CNN). We evaluate our approach on publicly available benchmarks using LiDAR, RGBD, and CAD data. VoxNet achieves accuracy beyond the state of the art while labeling hundreds of instances per second.},
author = {Maturana, Daniel and Scherer, Sebastian},
doi = {10.1109/IROS.2015.7353481},
file = {:home/alven/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Maturana, Scherer - 2015 - VoxNet A 3D Convolutional Neural Network for real-time object recognition.pdf:pdf},
isbn = {9781479999941},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
mendeley-groups = {EE288,ML},
pages = {922--928},
publisher = {IEEE},
title = {{VoxNet: A 3D Convolutional Neural Network for real-time object recognition}},
volume = {2015-Decem},
year = {2015}
}
@article{Mitwally2004,
author = {Mitwally, Mohamed F. and Casper, Robert F.},
file = {:home/alven/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mitwally, Casper - 2004 - Using aromatese inhibitors to induce ovulation in breast Ca Survivors.pdf:pdf},
issn = {00903159},
journal = {Contemporary Ob/Gyn},
mendeley-groups = {EE288,ML},
number = {1},
pages = {73--83},
title = {{Using aromatese inhibitors to induce ovulation in breast Ca Survivors}},
volume = {49},
year = {2004}
}
@article{Wang2019,
abstract = {A common approach to tackle 3D object recognition tasks is to project 3D data to multiple 2D images. Projection only captures the outline of the object, and discards the internal information that may be crucial for the recognition. In this paper, we stay in 3D and concentrate on tapping the potential of 3D representations. We present NormalNet, a voxel-based convolutional neural network (CNN) designed for 3D object recognition. The network uses normal vectors of the object surfaces as input, which demonstrate stronger discrimination capability than binary voxels. We propose a reflection–convolution–concatenation (RCC) module to realize the conv layers, which extracts distinguishable features for 3D vision tasks while reducing the number of parameters significantly. We further improve the performance of NormalNet by combining two networks, which take normal vectors and voxels as input respectively. We carry out a series of experiments that validate the design of the network and achieve competitive performance in 3D object classification and retrieval tasks.},
author = {Wang, Cheng and Cheng, Ming and Sohel, Ferdous and Bennamoun, Mohammed and Li, Jonathan},
doi = {10.1016/j.neucom.2018.09.075},
file = {:home/alven/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2019 - NormalNet A voxel-based CNN for 3D object classification and retrieval.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {3D object classification,3D object retrieval,Convolutional neural network,Network fusion},
mendeley-groups = {EE288},
pages = {139--147},
publisher = {Elsevier B.V.},
title = {{NormalNet: A voxel-based CNN for 3D object classification and retrieval}},
url = {https://doi.org/10.1016/j.neucom.2018.09.075},
volume = {323},
year = {2019}
}
@article{Pittaras2017,
abstract = {Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU ({\$}\backslashapprox{\$} 2.5 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.},
archivePrefix = {arXiv},
arxivId = {1408.5093},
author = {Pittaras, Nikiforos and Markatopoulou, Foteini and Mezaris, Vasileios and Patras, Ioannis},
doi = {10.1007/978-3-319-51811-4_9},
eprint = {1408.5093},
file = {:home/alven/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pittaras et al. - 2017 - Volumetric and Multi-View CNNs for Object Classification on 3D Data Supplementary Material.pdf:pdf},
isbn = {9783319518107},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Concept detection,Deep learning,Visual analysis},
mendeley-groups = {EE288,ML},
pages = {102--114},
title = {{Volumetric and Multi-View CNNs for Object Classification on 3D Data Supplementary Material}},
volume = {10132 LNCS},
year = {2017}
}
@article{Liu2019,
abstract = {We present Point-Voxel CNN (PVCNN) for efficient, fast 3D deep learning. Previous work processes 3D data using either voxel-based or point-based NN models. However, both approaches are computationally inefficient. The computation cost and memory footprints of the voxel-based models grow cubically with the input resolution, making it memory-prohibitive to scale up the resolution. As for point-based networks, up to 80{\%} of the time is wasted on structuring the irregular data which have rather poor memory locality, not on the actual feature extraction. In this paper, we propose PVCNN that represents the 3D input data in points to reduce the memory consumption, while performing the convolutions in voxels to largely reduce the irregular data access and improve the locality. Our PVCNN model is both memory and computation efficient. Evaluated on semantic and part segmentation datasets, it achieves much higher accuracy than the voxel-based baseline with 10x GPU memory reduction; it also outperforms the state-of-the-art point-based models with 7x measured speedup on average. Remarkably, narrower version of PVCNN achieves 2x speedup over PointNet (an extremely efficient model) on part and scene segmentation benchmarks with much higher accuracy. We validate the general effectiveness of our PVCNN on 3D object detection: by replacing the primitives in Frustrum PointNet with PVConv, it outperforms Frustrum PointNet++ by 2.4{\%} mAP on average with 1.5x measured speedup and GPU memory reduction.},
archivePrefix = {arXiv},
arxivId = {1907.03739},
author = {Liu, Zhijian and Tang, Haotian and Lin, Yujun and Han, Song},
eprint = {1907.03739},
file = {:home/alven/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2019 - Point-Voxel CNN for Efficient 3D Deep Learning.pdf:pdf},
mendeley-groups = {EE288},
pages = {1--11},
title = {{Point-Voxel CNN for Efficient 3D Deep Learning}},
url = {http://arxiv.org/abs/1907.03739},
year = {2019}
}@article{Cicek2016,
abstract = {This paper introduces a network for volumetric segmentation that learns from sparsely annotated volumetric images. We outline two attractive use cases of this method: (1) In a semi-automated setup,the user annotates some slices in the volume to be segmented. The network learns from these sparse annotations and provides a dense 3D segmentation. (2) In a fully-automated setup,we assume that a representative,sparsely annotated training set exists. Trained on this data set,the network densely segments new volumetric images. The proposed network extends the previous u-net architecture from Ronneberger et al. by replacing all 2D operations with their 3D counterparts. The implementation performs on-the-fly elastic deformations for efficient data augmentation during training. It is trained end-to-end from scratch,i.e.,no pre-trained network is required. We test the performance of the proposed method on a complex,highly variable 3D structure,the Xenopus kidney,and achieve good results for both use cases.},
archivePrefix = {arXiv},
arxivId = {1606.06650},
author = {{\c{C}}i{\c{c}}ek, {\"{O}}zg{\"{u}}n and Abdulkadir, Ahmed and Lienkamp, Soeren S. and Brox, Thomas and Ronneberger, Olaf},
doi = {10.1007/978-3-319-46723-8_49},
eprint = {1606.06650},
file = {:home/alven/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/{\c{C}}i{\c{c}}ek et al. - 2016 - 3D U-net Learning dense volumetric segmentation from sparse annotation.pdf:pdf},
isbn = {9783319467221},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {3D,Biomedical volumetric image segmentation,Convolutional neural networks,Fully-automated,Semi-automated,Sparse annotation,Xenopus kidney},
mendeley-groups = {EE288},
pages = {424--432},
title = {{3D U-net: Learning dense volumetric segmentation from sparse annotation}},
volume = {9901 LNCS},
year = {2016}
}
@article{Qi2016,
abstract = {3D shape models are becoming widely available and easier to capture, making available 3D information crucial for progress in object classification. Current state-of-theart methods rely on CNNs to address this problem. Recently, we witness two types of CNNs being developed: CNNs based upon volumetric representations versus CNNs based upon multi-view representations. Empirical results from these two types of CNNs exhibit a large gap, indicating that existing volumetric CNN architectures and approaches are unable to fully exploit the power of 3D representations. In this paper, we aim to improve both volumetric CNNs and multi-view CNNs according to extensive analysis of existing approaches. To this end, we introduce two distinct network architectures of volumetric CNNs. In addition, we examine multi-view CNNs, where we introduce multiresolution filtering in 3D. Overall, we are able to outperform current state-of-the-art methods for both volumetric CNNs and multi-view CNNs. We provide extensive experiments designed to evaluate underlying design choices, thus providing a better understanding of the space of methods available for object classification on 3D data.},
archivePrefix = {arXiv},
arxivId = {1604.03265},
author = {Qi, Charles R. and Su, Hao and Niebner, Matthias and Dai, Angela and Yan, Mengyuan and Guibas, Leonidas J.},
doi = {10.1109/CVPR.2016.609},
eprint = {1604.03265},
file = {:home/alven/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Qi et al. - 2016 - Volumetric and multi-view CNNs for object classification on 3D data.pdf:pdf},
isbn = {9781467388504},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
mendeley-groups = {EE288},
pages = {5648--5656},
title = {{Volumetric and multi-view CNNs for object classification on 3D data}},
volume = {2016-Decem},
year = {2016}
}
@article{Miyagi2018,
abstract = {We propose a sliced voxel representation, which we call Sliced Square Voxels (SSV), based on LSTM (Long Short-Term Memory) and CNN (Convolutional Neural Network) for three-dimensional shape recognition. Given an arbitrary 3D model, we first convert it into binary voxel of size 32×32×32. Then, after a view position is fixed, we slice the binary voxel data vertically in the depth direction. To utilize the 2D projected shape information of the sliced voxels, CNN has been applied. The output of CNN is fed into LSTM, which is our main idea, where the spatial topology is supposed to be favored with LSTM. From our experiments, our proposed method turns out to be superior to the baseline method which we prepared using 3DCNN. We further compared with related previous methods, using large-scale 3D model dataset (ModelNet10 and ModelNet40), and our proposed methods outperformed them.},
author = {Miyagi, Ryo and Aono, Masaki},
doi = {10.1109/APSIPA.2017.8282044},
file = {:home/alven/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Miyagi, Aono - 2018 - Sliced voxel representations with LSTM and CNN for 3D shape recognition.pdf:pdf},
isbn = {9781538615423},
journal = {Proceedings - 9th Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, APSIPA ASC 2017},
mendeley-groups = {EE288},
number = {December},
pages = {320--323},
title = {{Sliced voxel representations with LSTM and CNN for 3D shape recognition}},
volume = {2018-Febru},
year = {2018}
}
